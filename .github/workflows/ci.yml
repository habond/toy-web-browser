name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  quality-checks:
    name: Code Quality & Testing
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y fonts-dejavu-core fonts-liberation

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Cache pre-commit
      uses: actions/cache@v4
      with:
        path: ~/.cache/pre-commit
        key: pre-commit-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml') }}

    - name: Run pre-commit hooks
      run: |
        pre-commit run --all-files --show-diff-on-failure

    - name: Run comprehensive test suite
      run: |
        python -m pytest tests/ -v --cov=src --cov-report=xml --cov-report=term-missing

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: true

    - name: Test example rendering
      run: |
        ./scripts/examples.sh

    - name: Upload example outputs as artifacts
      uses: actions/upload-artifact@v4
      if: matrix.python-version == '3.11'
      with:
        name: example-outputs
        path: output_images/
        retention-days: 30

  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install bandit[toml] safety

    - name: Run Bandit security scan
      run: |
        bandit -r src/ -f json -o bandit-report.json
        bandit -r src/ -f txt

    - name: Check for known security vulnerabilities
      run: |
        safety check --output json > safety-report.json
        safety check

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y fonts-dejavu-core fonts-liberation

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark memory-profiler

    - name: Run performance benchmarks
      run: |
        mkdir -p output_images

        # Benchmark example rendering
        start_time=$(date +%s.%N)
        for i in {0..4}; do
          python -m src.browser examples/test1.html "output_images/perf_test_${i}.png"
        done
        end_time=$(date +%s.%N)

        # Calculate average time
        python -c "
        import sys
        start_time = float('$start_time')
        end_time = float('$end_time')
        avg_time = (end_time - start_time) / 5
        print(f'Average rendering time: {avg_time:.3f}s')

        # Fail if performance degrades significantly
        if avg_time > 5.0:
            print('WARNING: Rendering performance has degraded')
            sys.exit(1)
        else:
            print('✅ Performance test passed')
        "

    - name: Memory usage test
      run: |
        # Simple memory test using separate process measurement
        initial_memory=$(ps -o rss= -p $$)

        # Run the browser to render a complex example
        python -m src.browser examples/test3.html output_images/memory_test.png

        final_memory=$(ps -o rss= -p $$)
        memory_diff=$((final_memory - initial_memory))
        memory_mb=$((memory_diff / 1024))

        echo "Memory difference: ${memory_mb} MB"

        # Check if output file was created (basic success test)
        if [ -f "output_images/memory_test.png" ]; then
            echo "✅ Memory test passed - output file created"
        else
            echo "❌ Memory test failed - no output file"
            exit 1
        fi

  build-docs:
    name: Documentation Build
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Validate documentation
      run: |
        # Check that README and CLAUDE.md exist and are non-empty
        [ -s README.md ] && echo "✅ README.md exists and is not empty"
        [ -s CLAUDE.md ] && echo "✅ CLAUDE.md exists and is not empty"

        # Check for common documentation issues
        grep -q "TODO" README.md CLAUDE.md && echo "⚠️  Found TODO items in documentation" || echo "✅ No TODO items found"
        grep -q "FIXME" README.md CLAUDE.md && echo "⚠️  Found FIXME items in documentation" || echo "✅ No FIXME items found"

        # Validate that examples mentioned in docs exist
        if grep -q "examples/" README.md; then
            echo "✅ Examples directory referenced in README"
            ls examples/ > /dev/null 2>&1 && echo "✅ Examples directory exists"
        fi

    - name: Check documentation links
      run: |
        # Simple link validation for local references
        python -c "
        import re
        import os

        def check_file_links(filename):
            with open(filename, 'r') as f:
                content = f.read()

            # Find local file references
            local_refs = re.findall(r'\[.*?\]\(([^http][^)]+)\)', content)

            for ref in local_refs:
                if not ref.startswith('#'):  # Skip anchor links
                    if not os.path.exists(ref):
                        print(f'❌ Broken link in {filename}: {ref}')
                        return False
                    else:
                        print(f'✅ Valid link in {filename}: {ref}')
            return True

        readme_ok = check_file_links('README.md')
        claude_ok = check_file_links('CLAUDE.md')

        if not (readme_ok and claude_ok):
            exit(1)
        print('✅ All documentation links are valid')
        "

  compatibility-test:
    name: Platform Compatibility
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.10", "3.11"]
        exclude:
          # Reduce matrix size for faster builds
          - os: windows-latest
            python-version: "3.10"
          - os: macos-latest
            python-version: "3.10"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y fonts-dejavu-core fonts-liberation

    - name: Install system dependencies (macOS)
      if: matrix.os == 'macos-latest'
      run: |
        # macOS has fonts built-in, but we can install additional ones if needed
        echo "Using built-in macOS fonts"

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run core tests
      run: |
        python -m pytest tests/test_browser.py tests/test_html_parser.py tests/test_layout_engine.py -v

    - name: Test basic rendering (Unix)
      if: matrix.os != 'windows-latest'
      run: |
        python -m src.browser examples/test1.html output_test.png
        ls -la output_test.png  # Verify file was created

    - name: Test basic rendering (Windows)
      if: matrix.os == 'windows-latest'
      run: |
        python -m src.browser examples/test1.html output_test.png
        Get-ChildItem output_test.png  # Verify file was created

    - name: Cross-platform script test (Unix)
      if: matrix.os != 'windows-latest'
      run: |
        chmod +x scripts/render.sh
        ./scripts/render.sh examples/test1.html platform_test.png --output-dir output_images

    - name: Cross-platform script test (Windows)
      if: matrix.os == 'windows-latest'
      run: |
        python -m src.browser examples/test1.html platform_test.png
